# 决策树算法

## 决策树算法原理

决策树思想的来源非常朴素，程序设计中的`条件分支结构`就是if-then结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。

例如：
![决策树算法](https://raw.githubusercontent.com/zhusheng/blog/master/ml/19.png)

决策树的目的就是获得这样一个树形结构进行分类，当有一个新的样本过来了， 我们可以通过决策树进行分类。

那么哪个条件放在前面，哪个放在后面呢，这个就涉及到了`信息增益(消除不确定因素)`。每个特征（比如年龄、帅、收入、是否公务员）的信息增益，谁的信息增益最大，谁在前面，依次类推来绘制我们的结构树。

在了解信息增益之前，我们首先需要了解以下什么是信息熵。

### 信息熵

`信息熵`是`香农`在1948年发表的`《通信大的数学原理》论文`中首次提出的，认为`信息是可以被度量的`，信息的单位是`比特`。
![香农](https://raw.githubusercontent.com/zhusheng/blog/master/ml/20.png)

假设现在有32支球队，我们猜哪只球队会获得冠军。每猜一次给一块钱，告诉我是否猜对了，那么我需要掏多少钱才能知道谁是冠军？

我可以把球编上号，从1到32，然后提问：冠军在1-16号吗？依次询问，只需要五次，就可以知道结果。
![案例](https://raw.githubusercontent.com/zhusheng/blog/master/ml/21.png)

32支球队
$$ log_232=5比特 $$
64支球队
$$ log_264=6比特 $$

如果我们对32支球队一无所知，那么32支球队“谁是世界杯冠军”的信息量应该为5比特，实际“谁是世界杯冠军”的信息量应该比5比特少，因为我们肯定知道一些信息。香农指出，它的准确信息量应该是：

$$ H = -(P1log_2P1 + P2log_2P2 + ... + P32log_2P32) $$

H的专业术语称之为信息熵，单位为比特，公式如下：
$$ H(X) = \sum x \in X P(x)logP(x) $$

这32支球队夺冠的几率相同时，对应的信息熵等于5比特，计算结果如下：

$$ -\left( \frac{1}{32} log_2 \frac{1}{32} + \frac{1}{32} log_2 \frac{1}{32} + ... + \frac{1}{32} log_2 \frac{1}{32} \right) = - 32\times \left( \frac{1}{32} log_2 \frac{1}{32}\right) = -log_2\frac{1}{32} = -(-5)=5 $$

$$ log_2X = \frac{lnX}{ln2} = \frac{lgX}{lh2} $$

$$ lgX = log_{10}X $$

$$ lnX = log_eX $$

信息和消除不确定性是相联系的。
我们可以`通过信息增益来消除不确定性`，有了信息熵，我们就知道哪个的影响比较大，那个影响比较小，得出一个信息增益列表，这个信息增益列表就是我们划分决策树的依据。

### 信息增益

`信息增益是决策树的划分依据`。`信息增益的目的就是消除不确定性，哪个消除不确定性最多，就作为最先的节点`。

特征A对训练数据集D的信息增益g(D,A)，定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：

$$ g(D,A) = H(D) -H(D|A) $$

注：信息增益表示得知`特征X的信息`而使得`类别Y的信息`的`不确定性减少的程度`。

信息熵公式：

$$H(D) = -\sum_{k=1}^K \frac{|C_k|}{|D|} log\frac{|C_k|}{|D|}$$

条件熵公式：
$$H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|}H(D)_i = -\sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|}$$

注：$C_k$表示属于某个类别的样本数。

### 信息增益案例

样本数据如下：
![银行贷款样本数据](https://raw.githubusercontent.com/zhusheng/blog/master/ml/22.png)

上面的数据一共15个样本，4个特征（年龄、有工作、有自己的房子、信贷情况），2个类别（是、否）。

我们以A1表示年龄、A2表示有工作、A3表示有自己的房子、A4表示借贷情况，计算每个特征的信息熵：

(1)求$H(D)$

15个样本中，“是”有9个，“否”有6个，所以信息熵如下：

$$H(D)= -\left( \frac{9}{15}log_2\frac{9}{15} + \frac{6}{15}log_2\frac{6}{15}\right)=0.971$$

(2) 求$g(D|A_1)$，也就是$g(D|年龄)$

年龄共有3个类别，分别是“青年”、“中年”、“老年“，各有5个样本，所以:

$$H(D|A_1) =\frac{5}{15}H(青年)+\frac{5}{15}H(中年)+\frac{5}{15}H(老年)$$

5个青年样本中，类别“是”有3个样本，类别“否”有2个样本，所以：

$$ H(青年) = - \left(\frac{3}{5}log_2\frac{3}{5}  + \frac{2}{5}log_2\frac{2}{5}\right)=0.670$$

同理：

$$ H(中年) = - \left(\frac{3}{5}log_2\frac{3}{5}  + \frac{2}{5}log_2\frac{2}{5}\right)=0.670$$

$$ H(老年) = - \left(\frac{4}{5}log_2\frac{4}{5}  + \frac{1}{5}log_2\frac{1}{5}\right)=0.722$$

所以：

$$ g(D|A_1) = H(D) - H(D|A_1) = 0.971 - \frac{5}{15}\times0.670 - \frac{5}{15}\times0.670 - \frac{5}{15}\times0.722=0.283$$

(3)同理，我们求$g(D|A_2) 、g(D|A_3)、g(D|A_4)$

$$ g(D|A_2) = 0.324$$

$$ g(D|A_3) = 0.420$$

$$ g(D|A_4) = 0.363$$

## 常见的决策树算法

- ID3：信息增益最大的准则
- C4.5：信息增益比最大的准则
- CART
  - 回归树: 平方误差最小的原则
  - 分类树: 基尼系数最小的准则，在sklearn中决策树划分的默认原则。

## 数据集

我们使用泰坦尼克号数据集，为了方便，这里我们使用的是在线数据集。

关于泰坦尼克号旅客的数据的主要来源是百科全书Titanica。这里使用的数据集是由各种研究人员开始的。其中包括许多研究人员创建的旅客名单，由Michael A. Findlay编辑。
我们提取的数据集中的特征是票的类别，存活，乘坐班，年龄，登陆，home.dest，房间，票，船和性别。乘坐班是指乘客班（1，2，3），是社会经济阶层的代表。其中age数据存在缺失。

数据集地址为:"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"

数据集结构如下图所示：
![泰坦尼克数据集](https://raw.githubusercontent.com/zhusheng/blog/master/ml/01.png)
