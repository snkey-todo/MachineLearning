from sklearn.feature_extraction.text import CountVectorizer
import jieba


def cutword():
    c1 = jieba.cut("今天很残酷，明天更残酷，后天很美好,但绝对大部分是死在明天晚上，所以每个人不要放弃今天。")
    c2 = jieba.cut("我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。")
    c3 = jieba.cut("如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。")

    # 转换为词组列表
    c1 = list(c1)
    c2 = list(c2)
    c3 = list(c3)
    print(c1)

    # 转换为句子
    c1 = " ".join(c1)
    c2 = " ".join(c2)
    c3 = " ".join(c3)
    print(c1)

    return c1,c2,c3

def countvec():
    """
    对文本进行特征值化
    :return:
    """
    countvec = CountVectorizer()

    c1, c2, c3 = cutword()

    result = countvec.fit_transform([c1, c2, c3])

    print(countvec.get_feature_names())
    print(result.toarray())

if __name__ == "__main__":
    countvec()


"""
运行结果如下：
['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', ',', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']
今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 , 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。
['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '这样']
[[0 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 0]
 [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 1]
 [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0]]

 通过jieba分词，然后统计词出现的次数。
"""