# 线性回归

## 什么是线性回归

线性回归的公式：

1个特征：`y = w*x + b`， x是输入，y是输出，w是权重，b是偏置。

n个特征：`y = w1*x1 +  w2*x2 + ... + wn*xn + b`，有n个权重，1个偏置

线性回归的核心：

- 算法：线性回归。
- 策略：均方误差。
- 优化：梯度下降，需要设置学习率。

## 线性回归的使用步骤

1. `准备好特征值和目标值`，也就是数据。特征值是输入，目标值是输出。以预测房价为例，位置、面积、朝向等都是特征值，房总价是目标值。
2. `建立模型`。需要1个权重、1个偏置，然后进行随机初始化，得到一个预测函数`y_predict`。
3. `求损失函数loss`，也就是误差，我们以均方误差的方式来求损失。
4. 优化损失，在线性回归中，我们`通过梯度下降的方式来优化损失`，求损失函数最小值。形象化的说法：就是给定一个随机点，我们从山上找到山底的一个过程，如下图所示。

![image.png](https://upload-images.jianshu.io/upload_images/5637154-5af57ebbc5a69e9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

说明：什么是均方误差？

就是对预测值和目标值求差并开根号，然后求平均值，公式如下：

$$ \frac{[(y_1-y_1^{'})^2 + (y_2-y_2^{'})^2 + ... + (y_n-y_n^{'})^2]}{n}$$

## API

### 梯度下降

梯度下降优化API：`tf.train.GradientDescentOptimizer(learning_rate)`

参数说明：
learning_rate:学习率，一般为0.1、0.01、0.001等量级的，当然也可以是1、10、100但是基本不会这么写。

函数的返回值是一个梯度下降op，我们可以在会话中进行运行。

#### 学习率

Learning_rate在梯度下降中是一个非常重要的参数，它决定的是从山顶到达山底的step和速度。一般都是很小的，0.1、0.01、0.001等都有。

Learning_rate过大或者过小都不好，过大会导致梯度爆炸，过小会达不到想要的训练效果，需要增加训练次数。

#### 梯度爆炸

在极端的情况下，权重的值变得非常大，以致于溢出导致NAN值就是梯度爆炸。梯度爆炸问题在深度学习神经网络RNN中更容易出现。

如何解决梯度爆炸问题：

1. 重新设计网络；
2. 调整学习率；
3. 使用梯度截断，在训练过程中检查和限制梯度的大小；
4. 使用激活函数。


